# secure-llm-mia

Ready-to-run Google Colab project for evaluating membership inference attacks (MIAs) on continual fine-tuning of `meta-llama/Llama-3.1-8B` using chronological slices of clinical datasets.

## Quick Start
1. Launch Colab and open `notebooks/00_colab_setup.ipynb`.
2. Install dependencies from `env/requirements.txt`, authenticate with Hugging Face, and mount Google Drive (the repo is cloned into `/content/drive/MyDrive/secure-llm-mia`).
3. Upload `mimic-iv-bhc/mimic-iv-bhc.csv` to Drive (default path: `/content/drive/MyDrive/mimic-iv-bhc/mimic-iv-bhc.csv`), update `configs/data.yaml` if your layout differs, and rerun notebooks 01–03.
4. Iterate through notebooks 04–11 for fine-tuning, attack computation, and reporting. Use `12_run_sweep_driver.ipynb` or `scripts/run_*` as automation stubs once verified.

## Datasets & Guardrails
- Candidate corpora: MIMIC-IV-Ext-22MCTS (time-series), MIMIC-IV-Ext-BHC (summaries), MIMIC-IV v3.1 (hosp).
- The BHC CSV is loaded directly from Drive via `BHC_CSV_PATH` (default `/content/drive/MyDrive/mimic-iv-bhc/mimic-iv-bhc.csv`) using `src/data/bhc.py`.
- Access requires PhysioNet credentialing and institutional approvals; never sync PHI into the repo.
- Store raw data and intermediate text on encrypted Drive folders only. Use the config flag `disable_exports` (add to `configs/data.yaml`) to prevent accidental artifact syncing.
- Paraphrase generation logs rejected candidates to support human review before export.

## Run Modes (Subset vs Full)
- Controlled via the environment variable `SECURE_LLM_MIA_RUN_MODE` (`subset` by default, `full` for large-scale experiments).
- Subset mode now materializes **30k BHC examples** and caps continual fine-tuning to **4 slices × ≈3 M tokens** (≈1k training docs per slice); canonical/packed artifacts are suffixed with the run-mode name (e.g., `canonical_bhc_subset.parquet`).
- Full mode processes all 270k rows—set `SECURE_LLM_MIA_RUN_MODE=full` before running notebook `00_colab_setup.ipynb` to propagate the setting through downstream notebooks and scripts.
- `src/utils/runtime.py` centralizes the configuration so every notebook shares the same `PROJECT_ROOT`, `ARTIFACTS_DIR`, and run-mode-specific paths.

## Continual Fine-tuning Pipeline
- Slices: `T=4` chronological windows (quarterly) with token budget `≈3 M` per slice in subset mode (extendable for full runs).
- Token accounting: keep `tokens_per_step = 128k` via `micro_batch × grad_accum × avg_tokens/sample`.
- Replay: run tracks `{noreplay, replay10}` (10% replay) with shared seeds (`[13, 17, 23]`).
- Outputs: checkpoints under `checkpoints/slice_{t}/{track}/` plus member/non-member IDs in `artifacts/slice_{t}/ids/`.

## Membership Inference Suite
1. Loss/confidence baselines: mean NLL, entropy, max probability.
2. Win-k / Min-k% token-level statistics.
3. Label-free/self-prompt calibration (ref. Self-prompt Calibration 2311.06062).
4. Paraphrase robustness via back-translation or synonym swaps (500-member subset).
5. Ensemble attacks (logistic regression, optional XGBoost) over stacked features.
6. LiRA with base vs fine-tuned checkpoints, including temporal LLR trajectories and paraphrase stability.
7. Optional adaptive/stability probes for repeated querying.

## Evaluations & Reporting
- Panels per slice: 1k members, 1k global non-members, +500 past members, +500 future non-members.
- Metrics: ROC-AUC, TPR@1% FPR, ECE, Win-k curves, paraphrase stability, temporal leakage decay.
- Statistics: Bootstrap with 2,000 resamples, DeLong ROC comparisons, linear/mixed-effects trend vs time/replay.
- Reports: structured outputs in `reports/results.csv`, `reports/figs/`, `reports/tables/` (LaTeX-ready).

## Recommended Workflow
- Use notebooks sequentially; each notebook relies on artifacts generated by its predecessors.
- Validate synthetic smoke tests before touching real data.
- Document configuration changes in the YAML files and commit them for reproducibility.
- Prefer running expensive steps (tokenization, training, evaluation) in Colab Pro/Enterprise with A100 or T4 GPUs.

## Interpreting Temporal Leakage Plots
- `ROC` & `TPR@1%FPR`: primary leakage indicators per slice/track.
- `LLR_t` trajectories (from LiRA): examine drift across time and replay buckets.
- Paraphrase variance bars: signal how resilient leakage remains under semantic perturbations.
- Use DeLong p-values and bootstrap CIs to differentiate signal from noise; annotate plots accordingly.

## Automation Stubs
- `scripts/run_finetune_slice.sh` — customize to run notebook 04 headlessly.
- `scripts/run_attacks.sh` — chain notebooks 05–10 per slice.
- `scripts/export_figures.sh` — regenerate notebook 11 outputs.
- Extend `12_run_sweep_driver.ipynb` to orchestrate full T=8 × replay × seeds sweeps once artifacts are validated.

## Next Steps (TODOs)
- [ ] Replace synthetic loaders with secure MIMIC extraction pipelines.
- [ ] Implement actual QLoRA training loop with `accelerate` + PEFT.
- [ ] Integrate paraphrase generation with translation APIs/models vetted for privacy.
- [ ] Populate `reports/results.csv` via notebook 10 and craft publication-ready figures (notebook 11).
- [ ] Capture `pip freeze` after a validated Colab run for reproducibility tracking.
