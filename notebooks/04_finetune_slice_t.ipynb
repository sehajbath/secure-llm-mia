{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a13937d2",
      "metadata": {
        "id": "a13937d2"
      },
      "source": [
        "# 04 Â· Fine-tune Slice *t* (Unsloth QLoRA)\n",
        "\n",
        "Load 4-bit Llama weights, configure adapters, and respect the 25M token budget per slice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ktFoOYieIZtr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ktFoOYieIZtr",
        "outputId": "73407557-ef51-4a9b-b265-3a34317785d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.12/dist-packages (2025.11.3)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.11.4 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2025.11.4)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.24.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.9.35)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.0.33.post1)\n",
            "Requirement already satisfied: bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.48.2)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.5.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.3.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\n",
            "Requirement already satisfied: trl!=0.19.0,<=0.23.0,>=0.18.2 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.1.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (0.22.1)\n",
            "Requirement already satisfied: torchao>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.4->unsloth) (0.14.1)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.4->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.4->unsloth) (11.3.0)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.4->unsloth) (0.19.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (1.8.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "028bcad0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "028bcad0",
        "outputId": "02b4a8de-aa0c-474e-b994-12d4d1ef143e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROJECT_ROOT: /content/drive/MyDrive/secure-llm-mia\n",
            "Active run mode: subset - 30k-example subset powering the 4-slice, 3M-token continual fine-tuning regime.\n",
            "BHC CSV path: /content/drive/MyDrive/mimic-iv-bhc/mimic-iv-bhc.csv\n"
          ]
        }
      ],
      "source": [
        "# Persistent Drive + run mode setup\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    DRIVE_MOUNT = Path('/content/drive')\n",
        "    if not DRIVE_MOUNT.exists():\n",
        "        drive.mount('/content/drive')\n",
        "except Exception as exc:  # pragma: no cover\n",
        "    print(f'Colab drive mount skipped: {exc}')\n",
        "\n",
        "if Path('/content/drive').exists():\n",
        "    DRIVE_ROOT = Path('/content/drive/MyDrive').resolve()\n",
        "else:\n",
        "    DRIVE_ROOT = Path.home().resolve()\n",
        "\n",
        "PROJECT_ROOT = DRIVE_ROOT / 'secure-llm-mia'\n",
        "if not PROJECT_ROOT.exists():\n",
        "    raise FileNotFoundError('Run 00_colab_setup.ipynb first to clone the repo on Drive.')\n",
        "\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "os.environ['SECURE_LLM_MIA_ROOT'] = str(PROJECT_ROOT)\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "from src.utils.runtime import current_run_mode\n",
        "\n",
        "RUN_MODE = current_run_mode()\n",
        "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
        "print('Active run mode:', RUN_MODE.name, '-', RUN_MODE.description)\n",
        "\n",
        "DATA_ROOT = PROJECT_ROOT / 'data'\n",
        "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
        "CHECKPOINT_ROOT = PROJECT_ROOT / 'checkpoints'\n",
        "for path in (DATA_ROOT, ARTIFACTS_DIR, CHECKPOINT_ROOT):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "BHC_DATA_DIR = DRIVE_ROOT / 'mimic-iv-bhc'\n",
        "BHC_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BHC_CSV_PATH = BHC_DATA_DIR / 'mimic-iv-bhc.csv'\n",
        "print('BHC CSV path:', BHC_CSV_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fbb8f91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fbb8f91",
        "outputId": "058070c3-b5e2-445a-f1ec-30f2374c29e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Loaded sliced rows: 4225\n",
            "Slice 1: 1020 training rows loaded (text).\n",
            "Slice 2: 1062 training rows loaded (text).\n",
            "Slice 3: 1094 training rows loaded (text).\n",
            "Slice 4: 1049 training rows loaded (text).\n",
            "Gradient accumulation: 43\n",
            "Max steps per slice: 24\n",
            "=== Track: noreplay ===\n",
            "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.3 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/1020 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7396172e8ed4bbc88981ac6bef74445"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from src.modeling.lora import LoRAHyperParams, compute_gradient_accumulation\n",
        "from src.modeling.train import TokenBudgetTracker\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 1. Load sliced TEXT dataset produced in notebook 02\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "SLICED_PATH = ARTIFACTS_DIR / f\"sliced_dataset_{RUN_MODE.name}.parquet\"\n",
        "if not SLICED_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing sliced text dataset {SLICED_PATH}. Run notebook 02 first.\"\n",
        "    )\n",
        "\n",
        "df_sliced = pd.read_parquet(SLICED_PATH)\n",
        "print(\"Loaded sliced rows:\", len(df_sliced))\n",
        "\n",
        "# Build slice -> dataset mapping (train only)\n",
        "slice_datasets: Dict[int, Dataset] = {}\n",
        "for sid in sorted(df_sliced[\"slice_id\"].unique()):\n",
        "    slice_df = df_sliced[(df_sliced[\"slice_id\"] == sid) & (df_sliced[\"split_tag\"] == \"train\")].copy()\n",
        "    ds = Dataset.from_pandas(slice_df, preserve_index=False)\n",
        "    if \"text\" not in ds.column_names:\n",
        "        raise ValueError(\"Dataset must contain a `text` column.\")\n",
        "    slice_datasets[int(sid)] = ds\n",
        "    print(f\"Slice {sid}: {len(ds)} training rows loaded (text).\")\n",
        "\n",
        "SLICES = sorted(slice_datasets.keys())\n",
        "TRACKS = [\"noreplay\", \"replay10\"]\n",
        "REPLAY_FRACTION = 0.10\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2. Compute token-budget-driven step counts\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "MODEL_NAME = os.getenv(\"UNSLOTH_MODEL_NAME\", \"unsloth/Llama-3.2-3B-bnb-4bit\")\n",
        "MAX_SEQ_LENGTH = 4096\n",
        "TOKENS_PER_SLICE = 3_000_000\n",
        "TOKENS_PER_STEP = 128_000\n",
        "MICRO_BATCH = 1\n",
        "AVG_TOKENS_PER_SAMPLE = 3000\n",
        "\n",
        "accum_steps = compute_gradient_accumulation(\n",
        "    TOKENS_PER_STEP, MICRO_BATCH, AVG_TOKENS_PER_SAMPLE\n",
        ")\n",
        "print(\"Gradient accumulation:\", accum_steps)\n",
        "\n",
        "max_steps = math.ceil(TOKENS_PER_SLICE / TOKENS_PER_STEP)\n",
        "print(\"Max steps per slice:\", max_steps)\n",
        "\n",
        "is_ampere_plus = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "use_bf16 = bool(is_ampere_plus and torch.cuda.is_bf16_supported())\n",
        "use_fp16 = torch.cuda.is_available() and not use_bf16\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 3. Unsloth model initializer\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "def init_model():\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=MODEL_NAME,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    tokenizer.model_max_length = MAX_SEQ_LENGTH\n",
        "\n",
        "    lora_cfg = LoRAHyperParams(\n",
        "        r=32,\n",
        "        alpha=32,\n",
        "        dropout=0.0,\n",
        "        target_modules=(\n",
        "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "            \"gate_proj\",\"up_proj\",\"down_proj\"\n",
        "        ),\n",
        "    )\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=lora_cfg.r,\n",
        "        target_modules=list(lora_cfg.target_modules),\n",
        "        lora_alpha=lora_cfg.alpha,\n",
        "        lora_dropout=lora_cfg.dropout,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_training(model)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 4. Training loop: continual finetuning across slices per track\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "for track in TRACKS:\n",
        "    print(f\"=== Track: {track} ===\")\n",
        "\n",
        "    # Initialise base model ONCE per track\n",
        "    model, tokenizer = init_model()\n",
        "\n",
        "    for slice_id in SLICES:\n",
        "        # Base slice data\n",
        "        base_ds = slice_datasets[slice_id]\n",
        "        train_ds = base_ds\n",
        "\n",
        "        # Optional replay: add a fraction of previous slices' data\n",
        "        if track == \"replay10\":\n",
        "            prior_slice_ids = [sid for sid in SLICES if sid < slice_id]\n",
        "            if prior_slice_ids:\n",
        "                combined = concatenate_datasets([slice_datasets[p] for p in prior_slice_ids])\n",
        "                replay_n = min(int(len(base_ds) * REPLAY_FRACTION), len(combined))\n",
        "                if replay_n > 0:\n",
        "                    replay_subset = combined.shuffle(seed=17).select(range(replay_n))\n",
        "                    train_ds = concatenate_datasets([base_ds, replay_subset])\n",
        "                    print(\n",
        "                        f\"Slice {slice_id}: added {replay_n} replay samples from previous slices.\"\n",
        "                    )\n",
        "\n",
        "        # Directory for this slice's checkpoint\n",
        "        output_dir = CHECKPOINT_ROOT / f\"slice_{slice_id}\" / track / RUN_MODE.name\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # SFT config (one run per slice, continuing from current model weights)\n",
        "        sft_config = SFTConfig(\n",
        "            output_dir=str(output_dir),\n",
        "            per_device_train_batch_size=MICRO_BATCH,\n",
        "            gradient_accumulation_steps=accum_steps,\n",
        "            max_seq_length=MAX_SEQ_LENGTH,\n",
        "            warmup_steps=max(1, int(0.1 * max_steps)),\n",
        "            max_steps=max_steps,\n",
        "            learning_rate=5e-5,\n",
        "            logging_steps=1,\n",
        "            save_steps=50,\n",
        "            bf16=use_bf16,\n",
        "            fp16=use_fp16,\n",
        "            optim=\"adamw_8bit\",\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "\n",
        "        trainer = SFTTrainer(\n",
        "            model=model,              # reuse the SAME model object\n",
        "            tokenizer=tokenizer,\n",
        "            train_dataset=train_ds,\n",
        "            args=sft_config,\n",
        "            dataset_text_field=\"text\",\n",
        "            packing=False,\n",
        "            dataset_num_proc=2,\n",
        "        )\n",
        "\n",
        "        print(f\"--- Training slice {slice_id} ({track}) ---\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Keep updated weights in `model` for the next slice\n",
        "        model = trainer.model\n",
        "\n",
        "        # Approx token accounting (for logging)\n",
        "        approx_tokens = len(train_ds) * AVG_TOKENS_PER_SAMPLE\n",
        "        print(f\"Slice {slice_id}: approx tokens consumed {approx_tokens:,}\")\n",
        "\n",
        "        # Save LoRA checkpoint for this slice\n",
        "        model.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        print(f\"Saved adapters + tokenizer to {output_dir}\")\n",
        "\n",
        "        del trainer\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # (optional) after all slices for this track:\n",
        "    del model, tokenizer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}