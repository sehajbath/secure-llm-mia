{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64c1080",
   "metadata": {},
   "source": [
    "# 00 · Colab Setup\n",
    "\n",
    "Set up dependencies, authentication, and guardrails before running downstream notebooks.\n",
    "\n",
    "> **Data Protection:** Access to MIMIC datasets requires PhysioNet credentials and human oversight. Never sync PHI to this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18196c71",
   "metadata": {},
   "source": [
    "## Guardrails\n",
    "- Use this Colab only within approved clinical research scopes.\n",
    "- Mount Google Drive with restricted sharing.\n",
    "- Update configuration flags to disable exporting raw text artifacts.\n",
    "- Double-check that only anonymized metrics leave the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebcd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from the repo environment spec\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "requirements_path = PROJECT_ROOT / 'env' / 'requirements.txt'\n",
    "if requirements_path.exists():\n",
    "    print('Installing dependencies...')\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-U', '-r', str(requirements_path)], check=False)\n",
    "else:\n",
    "    print('requirements.txt not found. Verify repo checkout or adjust path.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46406032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face Hub (token input is not stored)\n",
    "from getpass import getpass\n",
    "try:\n",
    "    from huggingface_hub import login\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(f'huggingface_hub not available: {exc}')\n",
    "else:\n",
    "    hf_token = getpass('Enter Hugging Face token (press ENTER to skip): ')\n",
    "    if hf_token:\n",
    "        login(token=hf_token, add_to_git_credential=True)\n",
    "    else:\n",
    "        print('Token not provided. Access to meta-llama/Llama-3.1-8B will fail if gated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: initialize Weights & Biases logging\n",
    "try:\n",
    "    import wandb\n",
    "    enable_wandb = False  # TODO: flip to True after configuring wandb settings.\n",
    "    if enable_wandb:\n",
    "        wandb.login()\n",
    "        wandb.init(project='secure-llm-mia', config={'notebook': '00_colab_setup'})\n",
    "    else:\n",
    "        print('W&B disabled by default. Set enable_wandb=True to log runs.')\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(f'wandb not available: {exc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd342a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive if executing inside Colab\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    DRIVE_ROOT = Path('/content/drive')\n",
    "    if not DRIVE_ROOT.exists():\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        print('Drive mounted at /content/drive.')\n",
    "    else:\n",
    "        print('Drive already mounted.')\n",
    "except ImportError:\n",
    "    print('Not running inside Google Colab; skipping Drive mount.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine runtime device capabilities\n",
    "import torch\n",
    "\n",
    "\n",
    "def describe_device() -> None:\n",
    "    if not torch.cuda.is_available():\n",
    "        print('CUDA not available. Colab may be on CPU-only instance.')\n",
    "        return\n",
    "    device = torch.cuda.get_device_name(0)\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(f'CUDA device: {device} (cc {capability})')\n",
    "    print(f'BF16 support: {torch.cuda.is_bf16_supported()}')\n",
    "    print(f'FP16 support: {torch.cuda.is_available()}')\n",
    "\n",
    "\n",
    "describe_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90414433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure deterministic seeds and 4-bit defaults\n",
    "from src.utils.seed import set_global_seed\n",
    "from src.constants import ensure_directories, DATA_CACHE_DIR, ARTIFACT_ROOT\n",
    "\n",
    "PROJECT_TEMP = PROJECT_ROOT / 'colab_temp'\n",
    "PROJECT_TEMP.mkdir(exist_ok=True)\n",
    "set_global_seed(17)\n",
    "ensure_directories()\n",
    "print(f'Data cache directory: {DATA_CACHE_DIR}')\n",
    "print(f'Artifact root: {ARTIFACT_ROOT}')\n",
    "os.environ['BITSANDBYTES_NOWELCOME'] = '1'  # ensures bnb does not print ASCII art"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca72ae3",
   "metadata": {},
   "source": [
    "✅ **Next steps:** Proceed to `01_data_intake_and_clean.ipynb`, update dataset paths, and run the synthetic smoke test before working with credentialed data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
