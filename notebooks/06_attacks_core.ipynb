{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3545a67",
   "metadata": {},
   "source": [
    "# 06 ¬∑ Core Attacks\n",
    "\n",
    "Compute loss/confidence, Win-k, and label-free scores using Drive-backed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b46fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistent Drive + run mode setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    DRIVE_MOUNT = Path('/content/drive')\n",
    "    if not DRIVE_MOUNT.exists():\n",
    "        drive.mount('/content/drive')\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(f'Colab drive mount skipped: {exc}')\n",
    "\n",
    "if Path('/content/drive').exists():\n",
    "    DRIVE_ROOT = Path('/content/drive/MyDrive').resolve()\n",
    "else:\n",
    "    DRIVE_ROOT = Path.home().resolve()\n",
    "\n",
    "PROJECT_ROOT = DRIVE_ROOT / 'secure-llm-mia'\n",
    "if not PROJECT_ROOT.exists():\n",
    "    raise FileNotFoundError('Run 00_colab_setup.ipynb first to clone the repo on Drive.')\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "os.environ['SECURE_LLM_MIA_ROOT'] = str(PROJECT_ROOT)\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "from src.utils.runtime import current_run_mode\n",
    "\n",
    "RUN_MODE = current_run_mode()\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('Active run mode:', RUN_MODE.name, '-', RUN_MODE.description)\n",
    "\n",
    "DATA_ROOT = PROJECT_ROOT / 'data'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "CHECKPOINT_ROOT = PROJECT_ROOT / 'checkpoints'\n",
    "for path in (DATA_ROOT, ARTIFACTS_DIR, CHECKPOINT_ROOT):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BHC_DATA_DIR = DRIVE_ROOT / 'mimic-iv-bhc'\n",
    "BHC_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BHC_CSV_PATH = BHC_DATA_DIR / 'mimic-iv-bhc.csv'\n",
    "print('BHC CSV path:', BHC_CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from src.attacks.loss_confidence import score_examples\n",
    "from src.attacks.win_k_min_k import aggregate_features\n",
    "from src.attacks.label_free import CalibrationPrompt, summarize_outputs\n",
    "from src.eval.metrics import auc_metrics, tpr_at_fpr, expected_calibration_error\n",
    "\n",
    "SLICE_ID = 1\n",
    "TRACK = 'noreplay'\n",
    "FEATURES_PATH = PROJECT_ROOT / 'reports' / 'features' / f'features_slice_{SLICE_ID}_{TRACK}_{RUN_MODE.name}.parquet'\n",
    "IDS_DIR = ARTIFACTS_DIR / f'slice_{SLICE_ID}' / 'ids'\n",
    "\n",
    "if not FEATURES_PATH.exists():\n",
    "    raise FileNotFoundError('Feature parquet missing. Run 05_eval_generation_and_logprobs.ipynb first.')\n",
    "\n",
    "features = Dataset.from_parquet(str(FEATURES_PATH))\n",
    "print('Loaded features:', len(features))\n",
    "\n",
    "labels = features['labels'] if 'labels' in features.column_names else None\n",
    "if labels is None:\n",
    "    raise ValueError('Feature dataset lacks `labels`. Ensure notebook 05 saved member/non-member tags.')\n",
    "\n",
    "nll = np.array(features['token_nll'], dtype=float)\n",
    "entropy = np.array(features['token_entropy'], dtype=float)\n",
    "max_prob = np.array(features['token_max_prob'], dtype=float)\n",
    "win_dict = {key: np.array(features[key], dtype=float) for key in features.column_names if key.startswith('win@')}\n",
    "\n",
    "loss_feats = score_examples(nll, entropy, max_prob)\n",
    "win_feats = aggregate_features(win_dict, nll, worst_percents=[0.05, 0.10])\n",
    "\n",
    "feature_df = pd.DataFrame({\n",
    "    'labels': labels,\n",
    "    **{k: v for k, v in loss_feats.items()},\n",
    "    **win_feats,\n",
    "})\n",
    "print(feature_df.head())\n",
    "\n",
    "auc_info = auc_metrics(feature_df['labels'], -feature_df['mean_nll'])\n",
    "calib = expected_calibration_error(feature_df['labels'], 1.0 / feature_df['mean_nll'])\n",
    "tpr = tpr_at_fpr(feature_df['labels'], -feature_df['mean_nll'], target_fpr=0.01)\n",
    "print('AUC:', auc_info['auc'])\n",
    "print('TPR@1%FPR:', tpr)\n",
    "print('ECE:', calib.ece)\n",
    "\n",
    "metrics_path = PROJECT_ROOT / 'reports' / f'metrics_core_slice_{SLICE_ID}_{TRACK}_{RUN_MODE.name}.json'\n",
    "metrics = {\n",
    "    'slice_id': SLICE_ID,\n",
    "    'track': TRACK,\n",
    "    'run_mode': RUN_MODE.name,\n",
    "    'auc': auc_info['auc'],\n",
    "    'tpr@1%fpr': tpr,\n",
    "    'ece': calib.ece,\n",
    "}\n",
    "metrics_path.write_text(json.dumps(metrics, indent=2))\n",
    "print('Saved metrics to', metrics_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e47ee",
   "metadata": {},
   "source": [
    "üìù Store aggregated metrics under `reports/metrics_core_slice_t.json` and keep member/non-member IDs synced from Drive artifacts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
