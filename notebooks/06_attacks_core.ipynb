{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b3545a67",
      "metadata": {
        "id": "b3545a67"
      },
      "source": [
        "# 06 \u00b7 Core Attacks\n",
        "\n",
        "Compute loss/confidence, Win-k, and label-free scores using Drive-backed features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80b46fd9",
      "metadata": {
        "id": "80b46fd9",
        "outputId": "0c392c88-a702-4c38-fc8f-93a18107bc29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROJECT_ROOT: /content/drive/MyDrive/secure-llm-mia\n",
            "Active run mode: subset - Quick debugging subset (<=2k rows) for lightweight Colab smoke tests.\n",
            "BHC CSV path: /content/drive/MyDrive/mimic-iv-bhc/mimic-iv-bhc.csv\n"
          ]
        }
      ],
      "source": [
        "# Persistent Drive + run mode setup\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    DRIVE_MOUNT = Path('/content/drive')\n",
        "    if not DRIVE_MOUNT.exists():\n",
        "        drive.mount('/content/drive')\n",
        "except Exception as exc:  # pragma: no cover\n",
        "    print(f'Colab drive mount skipped: {exc}')\n",
        "\n",
        "if Path('/content/drive').exists():\n",
        "    DRIVE_ROOT = Path('/content/drive/MyDrive').resolve()\n",
        "else:\n",
        "    DRIVE_ROOT = Path.home().resolve()\n",
        "\n",
        "PROJECT_ROOT = DRIVE_ROOT / 'secure-llm-mia'\n",
        "if not PROJECT_ROOT.exists():\n",
        "    raise FileNotFoundError('Run 00_colab_setup.ipynb first to clone the repo on Drive.')\n",
        "\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "os.environ['SECURE_LLM_MIA_ROOT'] = str(PROJECT_ROOT)\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "from src.utils.runtime import current_run_mode\n",
        "\n",
        "RUN_MODE = current_run_mode()\n",
        "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
        "print('Active run mode:', RUN_MODE.name, '-', RUN_MODE.description)\n",
        "\n",
        "DATA_ROOT = PROJECT_ROOT / 'data'\n",
        "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
        "CHECKPOINT_ROOT = PROJECT_ROOT / 'checkpoints'\n",
        "for path in (DATA_ROOT, ARTIFACTS_DIR, CHECKPOINT_ROOT):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "BHC_DATA_DIR = DRIVE_ROOT / 'mimic-iv-bhc'\n",
        "BHC_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BHC_CSV_PATH = BHC_DATA_DIR / 'mimic-iv-bhc.csv'\n",
        "print('BHC CSV path:', BHC_CSV_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b86e2194",
      "metadata": {
        "id": "b86e2194",
        "outputId": "ca8dc0d6-a680-4b6c-fa30-d083c77aef7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved aggregated features to /content/drive/MyDrive/secure-llm-mia/reports/features/features_core_slice_1_noreplay_subset.parquet\n",
            "AUC: 0.36\n",
            "TPR@1%FPR: 0.2\n",
            "Saved metrics to /content/drive/MyDrive/secure-llm-mia/reports/metrics_core_slice_1_noreplay_subset.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "from src.eval.metrics import auc_metrics, tpr_at_fpr\n",
        "\n",
        "FEATURES_DIR = PROJECT_ROOT / 'reports' / 'features'\n",
        "if not FEATURES_DIR.exists():\n",
        "    raise FileNotFoundError('Feature directory missing. Run notebook 05 before launching this notebook.')\n",
        "\n",
        "RUN_MODE_NAME = RUN_MODE.name\n",
        "\n",
        "\n",
        "def _feature_files() -> list[Path]:\n",
        "    pattern = f\"features_slice_*_{RUN_MODE_NAME}.parquet\"\n",
        "    return sorted(FEATURES_DIR.glob(pattern))\n",
        "\n",
        "\n",
        "def _parse_slice_track(path: Path) -> tuple[int, str]:\n",
        "    stem = path.stem  # features_slice_{slice}_{track}_{run_mode}\n",
        "    prefix = 'features_slice_'\n",
        "    suffix = f'_{RUN_MODE_NAME}'\n",
        "    if not stem.startswith(prefix) or not stem.endswith(suffix):\n",
        "        raise ValueError(f'Unexpected feature file name: {path.name}')\n",
        "    core = stem[len(prefix):-len(suffix)]\n",
        "    slice_str, track = core.split('_', 1)\n",
        "    return int(slice_str), track\n",
        "\n",
        "\n",
        "def _ragged_mean(values: list[np.ndarray]) -> np.ndarray:\n",
        "    return np.array([seq.mean() if seq.size else np.nan for seq in values])\n",
        "\n",
        "\n",
        "def _worst_percent(values: list[np.ndarray], percent: float) -> np.ndarray:\n",
        "    stats: list[float] = []\n",
        "    for seq in values:\n",
        "        if not seq.size:\n",
        "            stats.append(np.nan)\n",
        "            continue\n",
        "        k = max(1, int(np.ceil(seq.size * percent)))\n",
        "        sorted_seq = np.sort(seq)\n",
        "        stats.append(sorted_seq[-k:].mean())\n",
        "    return np.array(stats)\n",
        "\n",
        "\n",
        "feature_paths = _feature_files()\n",
        "if not feature_paths:\n",
        "    raise FileNotFoundError(f'No feature parquet files found in {FEATURES_DIR} for run mode `{RUN_MODE_NAME}`.')\n",
        "\n",
        "metrics_records: list[dict] = []\n",
        "\n",
        "for feature_path in feature_paths:\n",
        "    slice_id, track = _parse_slice_track(feature_path)\n",
        "    ds = Dataset.from_parquet(str(feature_path))\n",
        "    if len(ds) == 0:\n",
        "        print(f'Skipping slice {slice_id} ({track}) - empty feature dataset.')\n",
        "        continue\n",
        "\n",
        "    token_nll = [np.asarray(seq, dtype=float) for seq in ds['token_nll']]\n",
        "    token_entropy = [np.asarray(seq, dtype=float) for seq in ds['token_entropy']]\n",
        "    token_max_prob = [np.asarray(seq, dtype=float) for seq in ds['token_max_prob']]\n",
        "\n",
        "    win_keys = [name for name in ds.column_names if name.startswith('win@')]\n",
        "    win_stats = {key: [np.asarray(seq, dtype=float) for seq in ds[key]] for key in win_keys}\n",
        "\n",
        "    feature_df = pd.DataFrame(\n",
        "        {\n",
        "            'example_id': ds['example_id'],\n",
        "            'label': np.array(ds['label'], dtype=int),\n",
        "            'mean_nll': _ragged_mean(token_nll),\n",
        "            'entropy': _ragged_mean(token_entropy),\n",
        "            'max_prob': _ragged_mean(token_max_prob),\n",
        "            'min_loss_top_5pct': _worst_percent(token_nll, 0.05),\n",
        "            'min_loss_top_10pct': _worst_percent(token_nll, 0.10),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    for key, seqs in win_stats.items():\n",
        "        feature_df[f\"win_frac_{key.split('@')[1]}\"] = _ragged_mean(seqs)\n",
        "\n",
        "    aggregated_path = feature_path.with_name(f'features_core_slice_{slice_id}_{track}_{RUN_MODE_NAME}.parquet')\n",
        "    feature_df.to_parquet(aggregated_path, index=False)\n",
        "    print(f'Saved aggregated features to {aggregated_path}')\n",
        "\n",
        "    scores = -feature_df['mean_nll'].to_numpy()\n",
        "    labels = feature_df['label'].to_numpy()\n",
        "    auc_info = auc_metrics(labels, scores)\n",
        "    tpr = tpr_at_fpr(labels, scores, target_fpr=0.01)\n",
        "\n",
        "    metrics_path = PROJECT_ROOT / 'reports' / f'metrics_core_slice_{slice_id}_{track}_{RUN_MODE_NAME}.json'\n",
        "    metrics = {\n",
        "        'slice_id': slice_id,\n",
        "        'track': track,\n",
        "        'run_mode': RUN_MODE_NAME,\n",
        "        'auc': float(auc_info['auc']),\n",
        "        'tpr_at_0.01': float(tpr),\n",
        "        'num_examples': int(len(feature_df)),\n",
        "    }\n",
        "    metrics_path.write_text(json.dumps(metrics, indent=2))\n",
        "    print(f'Saved metrics to {metrics_path}')\n",
        "    metrics_records.append(metrics)\n",
        "\n",
        "if not metrics_records:\n",
        "    raise RuntimeError('No metrics computed. Ensure feature parquet files are populated.')\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_records).sort_values(['track', 'slice_id'])\n",
        "metrics_csv = PROJECT_ROOT / 'reports' / f'metrics_core_{RUN_MODE_NAME}.csv'\n",
        "metrics_df.to_csv(metrics_csv, index=False)\n",
        "print(f'Wrote consolidated metrics to {metrics_csv}')\n",
        "\n",
        "results_long = metrics_df.melt(\n",
        "    id_vars=['slice_id', 'track', 'run_mode'],\n",
        "    value_vars=['auc', 'tpr_at_0.01'],\n",
        "    var_name='metric',\n",
        "    value_name='value',\n",
        ")\n",
        "results_path = PROJECT_ROOT / 'reports' / f'results_core_{RUN_MODE_NAME}.csv'\n",
        "results_long.to_csv(results_path, index=False)\n",
        "print(f'Saved tidy metrics table to {results_path}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}