{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07c3c79",
   "metadata": {},
   "source": [
    "# 06 Â· Core Attacks\n",
    "\n",
    "Compute loss/confidence, Win-k, and label-free scores using Drive-backed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c95971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistent project setup on Drive\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_ROOT = Path('/content/drive')\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not DRIVE_ROOT.exists():\n",
    "        drive.mount('/content/drive')\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(f'Colab drive mount skipped: {exc}')\n",
    "\n",
    "if DRIVE_ROOT.exists():\n",
    "    BASE_ROOT = (DRIVE_ROOT / 'MyDrive').resolve()\n",
    "else:\n",
    "    BASE_ROOT = Path.home().resolve()\n",
    "\n",
    "PROJECT_ROOT = BASE_ROOT / 'secure-llm-mia'\n",
    "if not PROJECT_ROOT.exists():\n",
    "    raise FileNotFoundError('Clone the repo via 00_colab_setup.ipynb before running this notebook.')\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "os.environ['SECURE_LLM_MIA_ROOT'] = str(PROJECT_ROOT)\n",
    "\n",
    "DATA_ROOT = PROJECT_ROOT / 'data'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "CHECKPOINT_ROOT = PROJECT_ROOT / 'checkpoints'\n",
    "for path in (DATA_ROOT, ARTIFACTS_DIR, CHECKPOINT_ROOT):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.attacks.loss_confidence import score_examples\n",
    "from src.attacks.win_k_min_k import aggregate_features\n",
    "from src.attacks.label_free import CalibrationPrompt, summarize_outputs\n",
    "\n",
    "# Placeholder synthetic inputs; replace with outputs from notebook 05\n",
    "rng = np.random.default_rng(1)\n",
    "BATCH = 32\n",
    "TOKENS = 16\n",
    "nll = rng.random((BATCH, TOKENS))\n",
    "entropy = rng.random((BATCH, TOKENS))\n",
    "max_prob = rng.random((BATCH, TOKENS))\n",
    "win_dict = {f'win@{k}': (rng.random((BATCH, TOKENS)) > 0.5) for k in (1, 5, 10, 20)}\n",
    "\n",
    "loss_features = score_examples(nll, entropy, max_prob)\n",
    "win_features = aggregate_features(win_dict, nll, worst_percents=[0.05, 0.10])\n",
    "\n",
    "prompt = CalibrationPrompt()\n",
    "responses = ['Synthetic response' for _ in range(BATCH)]\n",
    "label_free_scores = summarize_outputs([prompt.format('context')] * BATCH, responses)\n",
    "\n",
    "print('Loss features keys:', loss_features.keys())\n",
    "print('Win-k feature sample:', {k: v[:3] for k, v in win_features.items()})\n",
    "print('Label-free scores sample:', label_free_scores[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f0d7d",
   "metadata": {},
   "source": [
    "Store aggregated metrics under `reports/metrics_core_slice_t.json` and keep member/non-member IDs synced from Drive artifacts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
