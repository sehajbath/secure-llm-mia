{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3545a67",
   "metadata": {},
   "source": [
    "# 06 ¬∑ Core Attacks\n",
    "\n",
    "Compute loss/confidence, Win-k, and label-free scores using Drive-backed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b46fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistent Drive + run mode setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    DRIVE_MOUNT = Path('/content/drive')\n",
    "    if not DRIVE_MOUNT.exists():\n",
    "        drive.mount('/content/drive')\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(f'Colab drive mount skipped: {exc}')\n",
    "\n",
    "if Path('/content/drive').exists():\n",
    "    DRIVE_ROOT = Path('/content/drive/MyDrive').resolve()\n",
    "else:\n",
    "    DRIVE_ROOT = Path.home().resolve()\n",
    "\n",
    "PROJECT_ROOT = DRIVE_ROOT / 'secure-llm-mia'\n",
    "if not PROJECT_ROOT.exists():\n",
    "    raise FileNotFoundError('Run 00_colab_setup.ipynb first to clone the repo on Drive.')\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "os.environ['SECURE_LLM_MIA_ROOT'] = str(PROJECT_ROOT)\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "from src.utils.runtime import current_run_mode\n",
    "\n",
    "RUN_MODE = current_run_mode()\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('Active run mode:', RUN_MODE.name, '-', RUN_MODE.description)\n",
    "\n",
    "DATA_ROOT = PROJECT_ROOT / 'data'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "CHECKPOINT_ROOT = PROJECT_ROOT / 'checkpoints'\n",
    "for path in (DATA_ROOT, ARTIFACTS_DIR, CHECKPOINT_ROOT):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BHC_DATA_DIR = DRIVE_ROOT / 'mimic-iv-bhc'\n",
    "BHC_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BHC_CSV_PATH = BHC_DATA_DIR / 'mimic-iv-bhc.csv'\n",
    "print('BHC CSV path:', BHC_CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from src.eval.metrics import auc_metrics, expected_calibration_error, tpr_at_fpr\n",
    "\n",
    "SLICE_ID = 1\n",
    "TRACK = 'noreplay'\n",
    "FEATURES_PATH = PROJECT_ROOT / 'reports' / 'features' / f'features_slice_{SLICE_ID}_{TRACK}_{RUN_MODE.name}.parquet'\n",
    "if not FEATURES_PATH.exists():\n",
    "    raise FileNotFoundError('Feature parquet missing. Run notebook 05 to generate it.')\n",
    "\n",
    "features = Dataset.from_parquet(str(FEATURES_PATH))\n",
    "example_ids = features['example_id']\n",
    "labels = np.array(features['label'], dtype=int)\n",
    "\n",
    "if len(labels) == 0:\n",
    "    raise ValueError('No evaluation records found in feature dataset.')\n",
    "\n",
    "nll_sequences = [np.array(seq, dtype=float) for seq in features['token_nll']]\n",
    "entropy_sequences = [np.array(seq, dtype=float) for seq in features['token_entropy']]\n",
    "max_prob_sequences = [np.array(seq, dtype=float) for seq in features['token_max_prob']]\n",
    "win_sequences = {\n",
    "    key: [np.array(seq, dtype=float) for seq in features[key]]\n",
    "    for key in features.column_names\n",
    "    if key.startswith('win@')\n",
    "}\n",
    "\n",
    "def ragged_mean(seqs):\n",
    "    return np.array([seq.mean() if len(seq) else np.nan for seq in seqs])\n",
    "\n",
    "def worst_percent_loss(seqs, percent):\n",
    "    stats = []\n",
    "    for seq in seqs:\n",
    "        if len(seq) == 0:\n",
    "            stats.append(np.nan)\n",
    "            continue\n",
    "        k = max(1, int(np.ceil(len(seq) * percent)))\n",
    "        sorted_seq = np.sort(seq)\n",
    "        stats.append(sorted_seq[-k:].mean())\n",
    "    return np.array(stats)\n",
    "\n",
    "mean_nll = ragged_mean(nll_sequences)\n",
    "mean_entropy = ragged_mean(entropy_sequences)\n",
    "mean_max_prob = ragged_mean(max_prob_sequences)\n",
    "\n",
    "win_features = {\n",
    "    f'win_frac_{key.split('@')[1]}': ragged_mean(seqs)\n",
    "    for key, seqs in win_sequences.items()\n",
    "}\n",
    "\n",
    "feature_df = pd.DataFrame({\n",
    "    'example_id': example_ids,\n",
    "    'label': labels,\n",
    "    'mean_nll': mean_nll,\n",
    "    'entropy': mean_entropy,\n",
    "    'max_prob': mean_max_prob,\n",
    "    **win_features,\n",
    "    'min_loss_top_5pct': worst_percent_loss(nll_sequences, 0.05),\n",
    "    'min_loss_top_10pct': worst_percent_loss(nll_sequences, 0.10),\n",
    "})\n",
    "\n",
    "feature_df_path = FEATURES_PATH.with_name(f'features_core_slice_{SLICE_ID}_{TRACK}_{RUN_MODE.name}.parquet')\n",
    "feature_df.to_parquet(feature_df_path, index=False)\n",
    "print('Saved aggregated features to', feature_df_path)\n",
    "\n",
    "scores = -feature_df['mean_nll'].to_numpy()\n",
    "auc_info = auc_metrics(labels, scores)\n",
    "tpr = tpr_at_fpr(labels, scores, target_fpr=0.01)\n",
    "calib = expected_calibration_error(labels, np.clip(feature_df['max_prob'].to_numpy(), 1e-6, 1 - 1e-6))\n",
    "print('AUC:', auc_info['auc'])\n",
    "print('TPR@1%FPR:', tpr)\n",
    "print('ECE:', calib.ece)\n",
    "\n",
    "metrics = {\n",
    "    'slice_id': SLICE_ID,\n",
    "    'track': TRACK,\n",
    "    'run_mode': RUN_MODE.name,\n",
    "    'auc': auc_info['auc'],\n",
    "    'tpr@1%fpr': tpr,\n",
    "    'ece': calib.ece,\n",
    "}\n",
    "metrics_path = PROJECT_ROOT / 'reports' / f'metrics_core_slice_{SLICE_ID}_{TRACK}_{RUN_MODE.name}.json'\n",
    "metrics_path.write_text(json.dumps(metrics, indent=2))\n",
    "print('Saved metrics to', metrics_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e47ee",
   "metadata": {},
   "source": [
    "üìù Store aggregated metrics under `reports/metrics_core_slice_t.json` and keep member/non-member IDs synced from Drive artifacts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
